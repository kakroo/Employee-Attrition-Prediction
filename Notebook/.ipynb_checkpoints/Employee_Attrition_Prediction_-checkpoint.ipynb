{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Attrition Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Import and suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load employee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = pd.read_csv(\".\\data.csv\", sep='\\t')\n",
    "emp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the data type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data.isnull().any()\n",
    "#No nulls present data is fairly clean\n",
    "#No need to insert synthetic data for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking basic health of data for outliers and other issues which can bring noise\n",
    "emp_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EmployeeCount , StandardHours, Over18 and EmployeeNumber are irrelevant, hence dropping these\n",
    "emp_data=emp_data.drop([\"EmployeeCount\",\"StandardHours\",\"Over18\",'EmployeeNumber'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Encode binary categorical data \n",
    "emp_data[\"Gender\"] = emp_data[\"Gender\"].astype('category')\n",
    "emp_data[\"OverTime\"] = emp_data[\"OverTime\"].astype('category')\n",
    "emp_data[\"Attrition\"] = emp_data[\"Attrition\"].astype('category')\n",
    "\n",
    "emp_data[\"Gender\"] = emp_data[\"Gender\"].cat.codes\n",
    "emp_data[\"OverTime\"] = emp_data[\"OverTime\"].cat.codes\n",
    "emp_data[\"Attrition\"] = emp_data[\"Attrition\"].cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refining our list of numerical variables\n",
    "numerical = [u'Age', u'DailyRate',  u'JobSatisfaction',\n",
    "       u'MonthlyIncome', u'PerformanceRating',\n",
    "        u'WorkLifeBalance', u'YearsAtCompany', u'Attrition']\n",
    "\n",
    "\n",
    "g = sns.pairplot(emp_data[numerical], hue='Attrition', palette='seismic', diag_kind = 'kde',diag_kws=dict(shade=True))\n",
    "g.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## list down all the features and categorize them\n",
    "emp_data = pd.get_dummies(emp_data, columns=[\"MaritalStatus\",\"EducationField\"])\n",
    "emp_data = pd.get_dummies(emp_data, columns=[\"Department\",\"JobRole\"])\n",
    "personal_features = ['Age','DistanceFromHome','Education','EducationField_Human Resources','EducationField_Life Sciences','EducationField_Marketing','EducationField_Medical','EducationField_Other','EducationField_Technical Degree','Gender','MaritalStatus_Divorced','MaritalStatus_Married','MaritalStatus_Single','NumCompaniesWorked']\n",
    "money_features = ['DailyRate','HourlyRate','MonthlyIncome','MonthlyRate','PercentSalaryHike','StockOptionLevel']\n",
    "satisfication_features = ['EnvironmentSatisfaction','JobSatisfaction','RelationshipSatisfaction','WorkLifeBalance']\n",
    "\n",
    "emp_data = pd.get_dummies(emp_data, columns=[\"BusinessTravel\"])\n",
    "perks_info_features = ['BusinessTravel_Travel_Rarely','BusinessTravel_Travel_Frequently','BusinessTravel_Non-Travel','JobInvolvement','OverTime','PerformanceRating']\n",
    "\n",
    "job_role_features = ['JobRole_Healthcare Representative','JobRole_Human Resources','JobRole_Laboratory Technician','JobRole_Manager','JobRole_Manufacturing Director','JobRole_Research Director','JobRole_Research Scientist','JobRole_Sales Executive','JobRole_Sales Representative']\n",
    "department_features = ['Department_Human Resources','Department_Research & Development','Department_Sales']\n",
    "employee_work_features = ['JobLevel','TotalWorkingYears','TrainingTimesLastYear','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\n",
    "emp_imp_features = job_role_features + department_features + employee_work_features + perks_info_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Normalize data (Feature Scaling) #TO bring data in a specific range\n",
    "#Improves performance and accuracy\n",
    "emp_data[\"MonthlyRateLog\"] = np.log1p(emp_data[\"MonthlyRate\"])\n",
    "emp_data[\"MonthlyIncomeLog\"] = np.log1p(emp_data[\"MonthlyIncome\"])\n",
    "emp_data[\"HourlyRateLog\"] = np.log1p(emp_data[\"HourlyRate\"])\n",
    "emp_data[\"DailyRateLog\"] = np.log1p(emp_data[\"DailyRate\"])\n",
    "\n",
    "money_log_features = ['DailyRateLog','HourlyRateLog','MonthlyIncomeLog','MonthlyRateLog','PercentSalaryHike','StockOptionLevel']\n",
    "\n",
    "all_featues = personal_features + money_log_features + satisfication_features + emp_imp_features + perks_info_features\n",
    "#TODO: find best features using\n",
    "#rfc.feature_importances_\n",
    "best_features = ['MonthlyIncome','Age','DailyRate','OverTime','MonthlyRate','TotalWorkingYears','HourlyRate','YearsAtCompany','YearsWithCurrManager','PercentSalaryHike','NumCompaniesWorked','DistanceFromHome','JobLevel','EnvironmentSatisfaction','RelationshipSatisfaction','JobInvolvement','YearsInCurrentRole','WorkLifeBalance','StockOptionLevel']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model train and model performance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,features,dataset):\n",
    "    # extract features from the dataset\n",
    "    X_train = dataset[features]\n",
    "    Y_train = dataset['Attrition']\n",
    "    \n",
    "    # spilt the data for training and testing\n",
    "    X_tr, X_te, Y_tr, Y_te = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "    # train the model\n",
    "    model.fit(X_tr, Y_tr)   \n",
    "    Y_pr = model.predict(X_te)\n",
    "    accuracy = model.score(X_te,Y_te)\n",
    "    print(\"Model accuracy score is \",accuracy)\n",
    "    return Y_te,Y_pr\n",
    "\n",
    "\n",
    "def measure_perform_matrix(Y_pr,Y_te):\n",
    "    confMat = confusion_matrix(Y_pr,Y_te)\n",
    "    TP=confMat[0][0]\n",
    "    FP=confMat[0][1]\n",
    "    FN=confMat[1][0]\n",
    "    TN=confMat[1][1]\n",
    "\n",
    "    print(\"True Positive :\",TP)\n",
    "    print(\"False Positive :\",FP)\n",
    "    print(\"False Negative :\",FN)\n",
    "    print(\"True Negative :\",TN)\n",
    "    \n",
    "    Acc = (TP + TN)/(TP+FP+FN+TN)\n",
    "    print(Acc)\n",
    "    \n",
    "    Sensitivity = TP/(TP + FN) \n",
    "    Specificity = TN/(TN + FP) \n",
    "\n",
    "    print(\"Sensitivity :\",Sensitivity)\n",
    "    print(\"Specificity :\",Specificity)\n",
    "    \n",
    "    Precision = TP/(TP+FP)\n",
    "    invRecall = 1/Sensitivity\n",
    "    invPrecision = 1/Precision\n",
    "    F1_Score = 2/(invRecall + invPrecision)\n",
    "    print(\"Precision : \",Precision)\n",
    "    print(\"F1 Score\",F1_Score)   \n",
    "    \n",
    "    \n",
    "    ##Computing false and true positive rates\n",
    "    fpr, tpr,_=roc_curve(Y_pr,Y_te,drop_intermediate=False)\n",
    "    plt.figure()\n",
    "    #plot the ROC curve\n",
    "    plt.plot(fpr, tpr, color='red',lw=2, label='ROC curve')\n",
    "    #Adding Random FPR and TPR\n",
    "    plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "    #Title and label\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('Total Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.show()\n",
    "\n",
    "#Train model \n",
    "def train_model_grad(model,features,dataset,val):\n",
    "    # extract features from the dataset\n",
    "    X_train = dataset[features]\n",
    "    Y_train = dataset['Attrition']\n",
    "    \n",
    "    # spilt the data for training and testing\n",
    "    X_tr, X_te, Y_tr, Y_te = train_test_split(X_train, Y_train, test_size=0.2,random_state=val)\n",
    "    # train the model\n",
    "    model.fit(X_tr, Y_tr)   \n",
    "    Y_pr = model.predict(X_te)\n",
    "    accuracy = model.score(X_te,Y_te) \n",
    "    print(\"Model accuracy score is \",accuracy)\n",
    "    return Y_te,Y_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lReg = LogisticRegression()\n",
    "Y_test, Y_predict= train_model_grad(lReg,all_featues,emp_data,26)\n",
    "measure_perform_matrix(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest=RandomForestClassifier(n_estimators=1000,criterion=\"gini\",max_depth=10,n_jobs=-1)\n",
    "Y_test, Y_predict = train_model_grad(forest,all_featues,emp_data,26)\n",
    "measure_perform_matrix(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:Bring Scaled Features as input \n",
    "neuNet = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(256,), max_iter=500,random_state=26)\n",
    "#y_te,y_pr = train_model(neuNet,all_featues,emp_data)\n",
    "y_te, y_pr = train_model_grad(neuNet,all_featues,emp_data,26)\n",
    "measure_perform_matrix(y_pr,y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = GradientBoostingClassifier(n_estimators=45)\n",
    "y_te,y_pr = train_model_grad(gradBoost,all_featues,emp_data,26)\n",
    "measure_perform_matrix(y_pr,y_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "y_te,y_pr = train_model_grad(knn,all_featues,emp_data,26)\n",
    "measure_perform_matrix(y_pr,y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "#parameters range\n",
    "params_range=[100,250,500,1000]\n",
    "params_depth=[2,3,5]\n",
    "param_grid={'n_estimators':params_range,\n",
    "           'criterion':['entropy','gini'],\n",
    "           'max_depth':params_depth}\n",
    "gridsearch=GridSearchCV(estimator=forest,\n",
    "                       param_grid=param_grid,\n",
    "                       scoring='accuracy',\n",
    "                       cv=5,\n",
    "                       n_jobs=1)\n",
    "\n",
    "##\n",
    "X_train = emp_data[all_featues]\n",
    "Y_train = emp_data['Attrition']\n",
    "    \n",
    "# spilt the data for training and testing\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "##\n",
    "gridsearch.fit(X_tr,y_tr)\n",
    "print(gridsearch.best_score_)\n",
    "print(gridsearch.best_params_)\n",
    "\n",
    "#running on the test set\n",
    "clf=gridsearch.best_estimator_\n",
    "clf.fit(X_tr,y_tr)\n",
    "print('Test accuracy : %.3f' %clf.score(X_te,y_te))\n",
    "\n",
    "svm=gridsearch.best_estimator_\n",
    "svm.fit(X_tr,y_tr)\n",
    "print('Test accuracy: %.3f' %svm.score(X_te,y_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing how model performs for diff values of neighbors\n",
    "#searching for optimal value of K in KNN\n",
    "X_train = emp_data[all_featues]\n",
    "Y_train = emp_data['Attrition']\n",
    "k_range= range(1,51)\n",
    "k_scores=[]\n",
    "for k in k_range:\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    scores=cross_val_score(knn,X_train,Y_train,cv=10,scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting k_range vs k_scores\n",
    "plt.plot(k_range,k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Steps followed to create above models:\n",
    "* **Data Wrangling** : Data was fairly clean with not specific outliers and missing values.\n",
    "* **Feature engineering**: Selecting features which help us maximize accuracy. As part of this activity we have:    \n",
    "> * Dropped some parameters as they were not not relevant and were bringing noise for eg EmployeeCount,StandardHours etc\n",
    "> * Also we scaled some of the features which had high range (this improved model accuracy and performance)\n",
    "* **Choosing a model** : We researched about different types of models to select the best one suitable for our problem.\n",
    "* **Training** : We trained the models which were selected as part of the above exercise. We found *Neural Network* performed the best.\n",
    "* **Evaluation** : Created custom function to evaluate different performance parameters of a given model for better evaluation\n",
    "* **Hyperparameter tuning** : On the selected model, we tuned the hyperparameters to get best performance metrics such as ROC curve and accuracy out of the given models\n",
    "* **Prediction**: The model with the best metrics was selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_model(filename,X_test,Y_test):\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, Y_test)\n",
    "    print('Model accuracy is ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save model to disk\n",
    "def save_model(model):\n",
    "    filename = 'SavedModel.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(neuNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = train_test_split(X_train, Y_train, test_size=0.2, random_state=26)\n",
    "filename = 'SavedModel.sav'\n",
    "X_tr.head()\n",
    "load_model(filename,X_te,Y_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
